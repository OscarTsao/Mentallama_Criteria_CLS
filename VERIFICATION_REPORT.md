# MentalLLaMA Encoder-Style NLI Implementation Verification Report

**Date**: 2025-11-16
**Repository**: Mentallama_Criteria_CLS
**Reviewer**: Claude Code Verification Bot
**Task**: Verify decoder‚Üíencoder (Gemma Encoder style) implementation with MentalLLaMA

---

## Executive Summary

This repository is **NOT YET COMPLIANT** with the decoder‚Üíencoder method described in the cited paper. The codebase contains specifications and planning documents but lacks the actual encoder-style implementation. This report provides:

- ‚úÖ **9/9 critical checks performed**
- ‚ùå **8/9 checks FAILED**
- ‚úÖ **1/9 checks PASSED** (data structure exists)
- üì¶ **Complete patches provided** for all failures
- üß™ **Unit tests generated** for verification
- üìù **Run instructions included**

---

## Detailed Check Results

### ‚úÖ CHECK 1: Decoder LM (MentalLLaMA) as Backbone - **FAIL**

**Status**: ‚ùå **FAIL**

**Expected**: Load `klyang/MentaLLaMA-chat-7B` using encoder-style wrapper, not causal LM.

**Found**:
- `src/Project/SubProject/models/model.py:14` uses generic `transformers.AutoModel.from_pretrained(model_name)`
- No explicit MentalLLaMA model loading
- No encoder-style configuration

**Evidence**:
```python
# Current implementation (model.py:11-15)
class Model(torch.nn.Module):
    def __init__(self, model_name: str, num_labels: int):
        super(Model, self).__init__()
        self.transformer = transformers.AutoModel.from_pretrained(model_name)
        self.classifier = torch.nn.Linear(self.transformer.config.hidden_size, num_labels)
```

**Issue**: Uses generic `AutoModel` which may load causal LM configuration. Should explicitly use encoder-only configuration.

---

### ‚úÖ CHECK 2: Attention Masking (Causal ‚Üí Bidirectional) - **FAIL**

**Status**: ‚ùå **FAIL**

**Expected**:
- Override LLaMA's default causal attention mask
- Implement full bidirectional attention (no triangular mask)
- Use attention_mask (1=token, 0=pad) without causal restriction

**Found**:
- No attention mask modification in codebase
- No custom attention implementation
- Relies on default HuggingFace behavior (which is causal for LLaMA)

**Evidence**:
```bash
$ rg "causal|causal_mask|lower_triangular|tril|torch.triu|make_causal" -n
# No results found in Python files
```

**Issue**: LLaMA models use causal attention by default. Without explicit override, the model will use triangular masking, preventing encoder-style bidirectional attention.

---

### ‚úÖ CHECK 3: Classifier Head (Pooler + MLP) - **PARTIAL FAIL**

**Status**: ‚ö†Ô∏è **PARTIAL FAIL**

**Expected**:
- Pooling mechanism (first token or mean pooling)
- Dropout layer (‚âà0.1)
- Linear classifier to num_labels
- No generate() or text parsing

**Found**:
```python
# Current implementation (model.py:17-21)
def forward(self, input_ids, attention_mask):
    outputs = self.transformer(input_ids=input_ids, attention_mask=attention_mask)
    pooled_output = outputs[1]  # ‚ùå WRONG: LLaMA has no pooler output
    logits = self.classifier(pooled_output)
    return logits
```

**Issues**:
1. ‚ùå Assumes `outputs[1]` exists (LLaMA doesn't have pooler_output)
2. ‚ùå No dropout before classifier
3. ‚úÖ Has linear classifier (good)
4. ‚úÖ No generate() usage (good)

---

### ‚úÖ CHECK 4: Dropout Placement & Rates - **FAIL**

**Status**: ‚ùå **FAIL**

**Expected**:
- Attention softmax output dropout ‚âà 0.1
- FFN output dropout ‚âà 0.1
- Classifier head dropout ‚âà 0.1

**Found**:
```bash
$ rg "Dropout\(|dropout_rate|attn_dropout|ffn_dropout" -n
# No matches in implementation files
```

**Evidence**:
- `classification_head` class has `dropout_prob` parameter but doesn't use it
- No dropout in `Model.forward()`
- No custom attention layers with dropout

**Issue**: Missing dropout regularization critical for preventing overfitting.

---

### ‚úÖ CHECK 5: Right-Padding & Attention_Mask Semantics - **FAIL**

**Status**: ‚ùå **FAIL**

**Expected**:
- Tokenizer configured with `padding_side="right"`
- attention_mask: 1=token, 0=pad
- Proper collate function for batching

**Found**:
- No tokenizer initialization code in repository
- No data collator implementation
- Dataset file is essentially empty (1 line)

**Evidence**:
```bash
$ wc -l src/Project/SubProject/data/dataset.py
1 src/Project/SubProject/data/dataset.py
```

---

### ‚úÖ CHECK 6: MentalLLaMA Tokenizer/Embeddings/Config - **FAIL**

**Status**: ‚ùå **FAIL**

**Expected**:
- `AutoTokenizer.from_pretrained("klyang/MentaLLaMA-chat-7B")`
- Embedding dimensions match MentalLLaMA (4096)
- Config confirms LLaMA architecture
- No Gemma or other model artifacts

**Found**:
- Only reference in documentation: `CLAUDE.md:114`
- No actual tokenizer loading code
- Only one tokenizer import in `scripts/register_model.py` (not used for training)

**Search Results**:
```bash
$ rg "AutoTokenizer" --type py
scripts/register_model.py:24:from transformers import AutoTokenizer
# No usage in core training code
```

---

### ‚úÖ CHECK 7: Supervised Classification Loss (CrossEntropy) - **FAIL**

**Status**: ‚ùå **FAIL**

**Expected**:
- `nn.CrossEntropyLoss()` or `nn.BCEWithLogitsLoss()` for binary classification
- Training loop computing loss from logits and labels
- NO LM loss (no next-token prediction)

**Found**:
- Train engine file is empty: `1 line`
- No loss function implementation
- Only documentation reference to `BCEWithLogitsLoss` in specs

**Evidence**:
```bash
$ wc -l src/Project/SubProject/engine/train_engine.py
1 src/Project/SubProject/engine/train_engine.py
```

---

### ‚úÖ CHECK 8: ReDSM5 ‚Üí NLI Data Conversion - **PARTIAL PASS**

**Status**: ‚ö†Ô∏è **PARTIAL PASS**

**Expected**:
- Convert ReDSM5 format to (premise, hypothesis, label) pairs
- Premise = sentence_text
- Hypothesis = DSM-5 criterion text
- Label = 1 if status='1', else 0

**Found**:
- ‚úÖ ReDSM5 data files exist: `redsm5_posts.csv`, `redsm5_annotations.csv`
- ‚úÖ DSM-5 criteria JSON exists: `data/DSM5/MDD_Criteira.json`
- ‚ùå No data preprocessing script
- ‚ùå No dataset loader implementation

**Data Structure** (‚úÖ Correct):
```csv
# redsm5_annotations.csv format:
post_id,sentence_id,sentence_text,DSM5_symptom,status,explanation

# Available criteria in MDD_Criteira.json:
A.1: Depressed mood
A.2: Anhedonia
A.3: Weight/appetite changes
A.4: Sleep issues
A.5: Psychomotor issues
A.6: Fatigue
A.7: Worthlessness/guilt
A.8: Cognitive issues
A.9: Suicidal ideation
```

---

### ‚úÖ CHECK 9: Unit Tests & Example Scripts - **FAIL**

**Status**: ‚ùå **FAIL**

**Expected**:
- Unit tests for model forward pass
- Shape tests (deterministic outputs)
- Attention mask tests
- Tokenization tests
- Example inference script

**Found**:
```bash
$ ls tests/
.gitkeep
# No test files
```

---

## Summary Table

| Check | Component | Status | Issue |
|-------|-----------|--------|-------|
| 1 | MentalLLaMA Backbone | ‚ùå FAIL | Generic AutoModel, no encoder config |
| 2 | Bidirectional Attention | ‚ùå FAIL | No attention mask override |
| 3 | Classifier Head | ‚ö†Ô∏è PARTIAL | Wrong pooling (outputs[1]), no dropout |
| 4 | Dropout | ‚ùå FAIL | No dropout layers implemented |
| 5 | Padding/Masking | ‚ùå FAIL | No tokenizer/collator implementation |
| 6 | MentalLLaMA Config | ‚ùå FAIL | No tokenizer loading code |
| 7 | Classification Loss | ‚ùå FAIL | No training loop implemented |
| 8 | NLI Data Conversion | ‚ö†Ô∏è PARTIAL | Data exists, no loader |
| 9 | Tests & Examples | ‚ùå FAIL | No tests or examples |

**Overall Compliance**: ‚ùå **NOT COMPLIANT** (1 partial pass, 8 failures)

---

## Root Cause Analysis

The repository is in **early planning stage**:
- ‚úÖ Comprehensive specifications exist (`specs/001-model-use-mentallam/`)
- ‚úÖ Data files present
- ‚ùå Implementation phase not started
- ‚ùå Core files are placeholders (1 line each)

**Key Missing Components**:
1. Encoder-style attention implementation
2. MentalLLaMA-specific model wrapper
3. Proper pooling for non-pooler models
4. Dropout regularization
5. Tokenizer & data pipeline
6. Training loop with classification loss
7. Unit tests

---

## Patches & Solutions

All patches are provided in the following files:
- `PATCH_01_encoder_model.py` - Complete encoder-style model
- `PATCH_02_data_pipeline.py` - NLI dataset loader
- `PATCH_03_train_engine.py` - Training loop with CrossEntropyLoss
- `PATCH_04_tests.py` - Comprehensive unit tests
- `PATCH_05_inference_example.py` - Deterministic inference example

See detailed patches in subsequent files.

---

## Recommendations

### Immediate Actions (P0)
1. ‚úÖ **Implement encoder-style attention** using custom LlamaModel wrapper
2. ‚úÖ **Fix pooling** to use `last_hidden_state[:, 0, :]` or mean pooling
3. ‚úÖ **Add dropout layers** (0.1) before classifier
4. ‚úÖ **Implement tokenizer** with right-padding
5. ‚úÖ **Create data loader** for NLI conversion

### High Priority (P1)
6. ‚úÖ **Implement training loop** with CrossEntropyLoss
7. ‚úÖ **Add unit tests** for all components
8. ‚úÖ **Create inference script** with deterministic output

### Medium Priority (P2)
9. Validate on small dataset split
10. Profile memory usage
11. Add integration tests

---

## Next Steps

1. **Apply patches** in order (01‚Üí05)
2. **Install dependencies**: `pip install -e '.[dev]'`
3. **Run tests**: `pytest tests/ -v`
4. **Verify inference**: `python examples/inference_example.py`
5. **Run training**: Use fixed train_engine.py

---

## Appendix: Search Commands Summary

```bash
# Model/backbone search
rg "from_pretrained|MentalLLaMA|MentaLLaMA|klyang|LlamaModel" -n
# ‚úÖ Found references in docs, ‚ùå not in implementation

# Attention masking search
rg "causal|causal_mask|lower_triangular|tril|torch.triu" -n
# ‚ùå No results (missing bidirectional attention)

# Loss function search
rg "CrossEntropyLoss|BCEWithLogitsLoss|nn.CrossEntropyLoss" -n
# ‚ùå Only in documentation

# Dropout search
rg "Dropout\(|dropout_rate|attn_dropout|ffn_dropout" -n
# ‚ùå No implementation

# Data conversion search
rg "redsm5|ReDSM5|DSM5|criterion|entailment" -n
# ‚úÖ Data files exist, ‚ùå no loader
```

---

**Report End**
